{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ZG0yu5F_ND",
   "metadata": {
    "id": "50ZG0yu5F_ND"
   },
   "source": [
    "### **Mental-Health Text Classification with Transformers (16 Classes) - Quick Introduction**\n",
    "\n",
    "Online communities and support forums host millions of mental-healthâ€“related posts every month. Tagging these posts by **topic** (e.g., *anxiety*, *PTSD*, *depression*) enables better moderation tools, trend monitoring, and routing to appropriate helpers. Most platforms cannot rely on keywords aloneâ€”language is nuanced, posts are long, and categories overlap.\n",
    "\n",
    "In this project, we aim to build an end-to-end text classifier using Hugging Face Transformers to categorize mental-health-related posts into one of 16 predefined themes:\n",
    "\n",
    "**adhd, healthanxiety, bipolarreddit, socialanxiety, bpd, normal, autism, ptsd, suicidewatch, depression, alcoholism, anxiety, lonely, EDAnonymous, schizophrenia, addiction.**\n",
    "\n",
    "This is a single-label multi-class classification task, where we will leverage pretrained Transformer encoder models with a classification head. The loss function used for training will be cross-entropy.\n",
    "\n",
    "To achieve this, we will be training and comparing the performance of three different Transformer models:\n",
    "- **RoBERTa-base**\n",
    "- **DeBERTa-v3-base**\n",
    "- **ModernBERT-large**\n",
    "\n",
    "By evaluating these models, we can determine which one is best suited for this specific text classification task.\n",
    "\n",
    "This notebook is organized into the following sections:\n",
    "\n",
    "1.  **Collab Setup**: Prepare your setup in collab.\n",
    "2.  **Packages Loading**: Load all necessary packages.\n",
    "3.  **Utility Functions**: Defining reusable functions used throughout the notebook.  \n",
    "4.  **Data Loading**: Loading the dataset and preparing it for model training, including tokenization.\n",
    "5.  **Models Definition, Configuration, Training, Inference on Unseen Data**: Describing the models used, their configurations, training and performances\n",
    "6.  **Results, Interpretation & Future Works**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e599e",
   "metadata": {
    "id": "106e599e"
   },
   "source": [
    "### **1. Collab Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to upload these three files :\n",
    "# - Your .env file containing your WANDB_API_KEY file\n",
    "# The poetry.lock file\n",
    "# The pyproject.toml file\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33632dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Poetry and any needed dependency\n",
    "!pip install poetry==2.2.1\n",
    "!poetry install --no-root\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7875837c",
   "metadata": {
    "id": "7875837c"
   },
   "source": [
    "### **2. Packages Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import inspect\n",
    "from dataclasses import dataclass, asdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from huggingface_hub import notebook_login : No needed for you since you don;t have access to push on the Hugging Face repo\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from huggingface_hub import HfApi\n",
    "from transformers.training_args import TrainingArguments as HFTrainingArguments\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import transformers # Keep this import for printing version\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"transformers\", transformers.__version__)\n",
    "print(\"TrainingArguments from:\", HFTrainingArguments.__module__)\n",
    "print(\"evaluate   \", evaluate.__version__)\n",
    "print(\"torch      \", torch.__version__, \"CUDA:\", torch.cuda.is_available()) # Corrected cuda check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0eb5f",
   "metadata": {
    "id": "0fb0eb5f"
   },
   "source": [
    "### **3. Utility Functions**\n",
    "\n",
    "This section contains reusable functions that are used throughout the notebook for various tasks such as data processing, model evaluation, and interacting with the Hugging Face Hub.\n",
    "\n",
    "By defining these functions separately, the main sections of the notebook remain cleaner and more focused on the core steps of the ML workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t_y7xsNwGZyy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Config\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name: str = \"roberta-large\"             # or microsoft/deberta-v3-base\n",
    "    max_length: int = 384\n",
    "    batch_size: int = 16\n",
    "    epochs: int = 5\n",
    "    lr: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "    scheduler: str = \"cosine\"\n",
    "    seed: int = 42\n",
    "    output_dir: str = \"mh_sent_model\"\n",
    "    report_to: str = \"none\"\n",
    "    label_smoothing: float = 0.05\n",
    "    save_total_limit: int = 2\n",
    "    save_safetensors: bool = True\n",
    "    eval_strategy: str = \"epoch\"\n",
    "    eval_steps: int = 200\n",
    "    logging_steps: int = 50\n",
    "    gradient_accumulation: int = 1\n",
    "    fp16: bool = True\n",
    "    use_early_stopping: bool = True\n",
    "    early_stopping_patience: int = 2\n",
    "    # Hugging Face Hub\n",
    "    hub_repo_id: Optional[str] = None\n",
    "    hub_private: bool = True\n",
    "    push_during_training: bool = False            # keep False; push best at end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ea3d5",
   "metadata": {
    "id": "da9ea3d5"
   },
   "source": [
    "##### _Some Quick Helper Functions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HgRpF9dEGZyz",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"Sets the seed for reproducibility across numpy, torch, and potentially Python's hash.\n",
    "\n",
    "    Ensures that random operations are deterministic for easier debugging and comparison\n",
    "    of results across different runs.\n",
    "    \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # Set Python hash seed\n",
    "    np.random.seed(seed) # Set numpy seed\n",
    "    torch.manual_seed(seed) # Set torch seed for CPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed) # Set torch seed for all GPUs\n",
    "    print(f\"Seed set to {seed}\") # Inform the user that the seed has been set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dceea8",
   "metadata": {
    "id": "43dceea8"
   },
   "source": [
    "##### _Data Processing Functions_\n",
    "\n",
    "They're functions responsible for preparing the loaded data for model training.\n",
    "\n",
    "**`create_datasets`**: This function takes the pandas DataFrames (`train_df`, `val_df`, `test_df`) and converts them into Hugging Face `Dataset` objects, organized within a `DatasetDict`. It also creates the necessary mappings between class labels (strings) and integer IDs (`label2id` and `id2label`) which are required for the classification models.\n",
    "\n",
    "**`tokenize_datasets`**: This function is used for converting the raw text data into a numerical format that Transformer models can process. It uses a tokenizer (compatible with the chosen model) to:\n",
    "*   Break down the text into tokens.\n",
    "*   Convert tokens into input IDs.\n",
    "*   Add attention masks and handle padding/truncation based on the specified `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc8faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    \"\"\"Creates a DatasetDict from dataframes.\"\"\"\n",
    "    classes = sorted(train_df[\"label\"].unique().tolist())\n",
    "    label2id = {lbl: i for i, lbl in enumerate(classes)}\n",
    "    id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "    num_labels = len(classes)\n",
    "\n",
    "    def to_ids(df):\n",
    "        df = df[[\"text\", \"label\"]].copy()\n",
    "        df[\"label\"] = df[\"label\"].map(label2id)\n",
    "        return df\n",
    "\n",
    "    raw = DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(to_ids(train_df), preserve_index=False),\n",
    "        \"validation\": Dataset.from_pandas(to_ids(val_df), preserve_index=False),\n",
    "        \"test\": Dataset.from_pandas(to_ids(test_df), preserve_index=False),\n",
    "    })\n",
    "\n",
    "    if \"label\" in raw[\"train\"].column_names:\n",
    "        raw = raw.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    return raw, label2id, id2label, num_labels\n",
    "\n",
    "def tokenize_datasets(raw_datasets: DatasetDict, model_name: str, max_length: int):\n",
    "    \"\"\"Tokenizes the datasets.\"\"\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, force_download=True)\n",
    "    collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tok(examples[\"text\"], truncation=True, max_length=max_length)\n",
    "\n",
    "    tok_ds = raw_datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    print(\"Tokenized datasets:\")\n",
    "    print(tok_ds)\n",
    "\n",
    "    return tok_ds, tok, collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748e4ec",
   "metadata": {
    "id": "1748e4ec"
   },
   "source": [
    "##### _Evaluation Functions_\n",
    "\n",
    "This section contains functions used to keep trained models' parameters, and evaluate their performance.\n",
    "\n",
    "**`compute_metrics`**: This function calculates various evaluation metrics (like accuracy, F1-score, precision, and recall) given the model's predictions and the true labels. It's designed to be used with the Hugging Face Trainer during evaluation.\n",
    "\n",
    "**`evaluate_model`**: This function takes a trained model (via the Trainer), the tokenized datasets, and label mappings to perform a comprehensive evaluation on a specified dataset split (e.g., the test set). It uses the `compute_metrics` function and also generates a classification report and a confusion matrix to visualize the model's performance across different classes.\n",
    "\n",
    "**`plot_loss_curves`**: This function visualizes the training and validation loss over the training steps. This helps in understanding the training progress and detecting potential issues like overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1402a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes metrics for evaluation.\"\"\"\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"macro_f1\": f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "        \"macro_prec\": precision_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
    "        \"macro_rec\": recall_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
    "    }\n",
    "\n",
    "def evaluate_model(trainer: Trainer, tokenized_datasets: DatasetDict, id2label: Dict[int, str], num_labels: int, split: str = \"test\"):\n",
    "    \"\"\"Evaluates the model on a specified dataset split and prints a classification report and confusion matrix.\"\"\"\n",
    "    if split not in tokenized_datasets:\n",
    "        print(f\"Error: Split '{split}' not found in tokenized_datasets.\")\n",
    "        return None, None\n",
    "\n",
    "    dataset_split = tokenized_datasets[split]\n",
    "\n",
    "    pred_logits = trainer.predict(dataset_split).predictions\n",
    "    y_pred = pred_logits.argmax(axis=1)\n",
    "    y_true = np.array(dataset_split[\"labels\"])\n",
    "\n",
    "    print(f\"Classification Report ({split.capitalize()} Data):\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=[id2label[i] for i in range(num_labels)],\n",
    "        digits=4,\n",
    "        output_dict=True\n",
    "    )\n",
    "    print(classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=[id2label[i] for i in range(num_labels)],\n",
    "        digits=4\n",
    "    ))\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=sorted(np.unique(y_true)))\n",
    "    label_names = [id2label[i] for i in sorted(np.unique(y_true))]\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"Confusion Matrix ({split.capitalize()} Data)\")\n",
    "    plt.show()\n",
    "\n",
    "    accuracy_split = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy on {split.capitalize()} Set: {accuracy_split:.4f}\")\n",
    "\n",
    "    return report, accuracy_split\n",
    "\n",
    "def plot_loss_curves(trainer_state_log_history):\n",
    "    \"\"\"Plots training and evaluation loss curves.\"\"\"\n",
    "    logs_df = pd.DataFrame(trainer_state_log_history)\n",
    "\n",
    "    def curve(df, y):\n",
    "        if not {'step', y}.issubset(df.columns): return pd.DataFrame(columns=['step', y])\n",
    "        return (df[['step', y]].dropna(subset=[y]).drop_duplicates('step').sort_values('step'))\n",
    "\n",
    "    train_loss_df = curve(logs_df, 'loss')\n",
    "    eval_loss_df  = curve(logs_df, 'eval_loss')\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    if not train_loss_df.empty:\n",
    "        plt.plot(train_loss_df['step'], train_loss_df['loss'], label='Training Loss')\n",
    "    if not eval_loss_df.empty:\n",
    "        plt.plot(eval_loss_df['step'],  eval_loss_df['eval_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Global step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def summarize_model_parameters(model, model_name: str):\n",
    "    \"\"\"Summarizes the parameters of a given PyTorch model.\"\"\"\n",
    "    param_data = []\n",
    "    for name, param in model.named_parameters():\n",
    "        param_data.append({\n",
    "            \"Layer Name\": name,\n",
    "            \"Shape\": tuple(param.shape),\n",
    "            \"Number of Parameters\": param.numel(),\n",
    "            \"Trainable\": param.requires_grad\n",
    "        })\n",
    "\n",
    "    params_df = pd.DataFrame(param_data)\n",
    "\n",
    "    total_params = params_df[\"Number of Parameters\"].sum()\n",
    "    trainable_params = params_df[params_df[\"Trainable\"]][\"Number of Parameters\"].sum()\n",
    "\n",
    "    print(f\"Total parameters ({model_name} Model): {total_params}\")\n",
    "    print(f\"Trainable parameters ({model_name} Model): {trainable_params}\")\n",
    "\n",
    "    print(f\"\\n{model_name} Model Parameters (by layer):\")\n",
    "    display(params_df)\n",
    "\n",
    "    # Add a breakdown by layer type\n",
    "    params_df['Layer Type'] = params_df['Layer Name'].apply(lambda x: x.split('.')[0])\n",
    "    layer_type_summary_df = params_df.groupby('Layer Type')['Number of Parameters'].sum().reset_index()\n",
    "    print(f\"\\nParameter Count by Layer Type ({model_name} Model):\")\n",
    "    display(layer_type_summary_df)\n",
    "\n",
    "    return params_df, layer_type_summary_df, total_params, trainable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b148772",
   "metadata": {
    "id": "5b148772"
   },
   "source": [
    "##### _Hugging Face Training and Utilities_\n",
    "\n",
    "This section contains functions that utilize the Hugging Face `transformers` and `accelerate` libraries to configure, train, and manage the models.\n",
    "\n",
    "**`get_training_args`**: This function translates the configuration settings defined in the `Config` dataclass into a `transformers.TrainingArguments` object, which is used to control the training process.\n",
    "\n",
    "**`train_model`**: This function initializes the model from a pretrained checkpoint, sets up the Hugging Face `Trainer` with the specified training arguments, and initiates the training process. It also incorporates the early stopping callback if enabled in the configuration.\n",
    "\n",
    "**`push_to_hub`**: This function handles saving the trained model and tokenizer and uploading them to the Hugging Face Hub, making them easily accessible for sharing and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_args(config: Config, num_labels: int, id2label: Dict[int, str], label2id: Dict[str, int]):\n",
    "    \"\"\"Gets TrainingArguments from Config.\"\"\"\n",
    "    common_kw = asdict(config)\n",
    "\n",
    "    sig = inspect.signature(HFTrainingArguments.__init__)\n",
    "    params = sig.parameters\n",
    "\n",
    "    kw = common_kw.copy()\n",
    "\n",
    "    if \"evaluation_strategy\" in params:\n",
    "        kw.update(\n",
    "            evaluation_strategy=config.eval_strategy,\n",
    "            save_strategy=config.eval_strategy,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"macro_f1\",\n",
    "            greater_is_better=True,\n",
    "            warmup_ratio=config.warmup_ratio,\n",
    "            logging_steps=config.logging_steps,\n",
    "            gradient_accumulation_steps=config.gradient_accumulation,\n",
    "            fp16=config.fp16,\n",
    "            report_to=config.report_to,\n",
    "            label_smoothing_factor=config.label_smoothing,\n",
    "        )\n",
    "        if \"save_total_limit\" in params:\n",
    "            kw[\"save_total_limit\"] = config.save_total_limit\n",
    "        if \"save_safetensors\" in params:\n",
    "            kw[\"save_safetensors\"] = config.save_safetensors\n",
    "    else:\n",
    "        kw.update(save_steps=1000)\n",
    "\n",
    "    if \"push_to_hub\" in params:\n",
    "        kw[\"push_to_hub\"] = config.push_during_training\n",
    "    if \"hub_model_id\" in params:\n",
    "        kw[\"hub_model_id\"] = config.hub_repo_id\n",
    "    if \"hub_private_repo\" in params:\n",
    "        kw[\"hub_private_repo\"] = config.hub_private\n",
    "\n",
    "    # Filter out arguments not supported by the current HFTrainingArguments version\n",
    "    valid_params = {k: v for k, v in kw.items() if k in params}\n",
    "\n",
    "    return HFTrainingArguments(**valid_params)\n",
    "\n",
    "def train_model(model_name: str, num_labels: int, id2label: Dict[int, str], label2id: Dict[str, int],\n",
    "                tokenized_datasets: DatasetDict, data_collator, config: Config, args: HFTrainingArguments):\n",
    "    \"\"\"Initializes and trains the model.\"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    callbacks = []\n",
    "    if config.use_early_stopping:\n",
    "        try:\n",
    "            callbacks = [EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)]\n",
    "        except Exception:\n",
    "            callbacks = []\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=callbacks,\n",
    "        tokenizer=AutoTokenizer.from_pretrained(model_name, use_fast=True), # Pass tokenizer here\n",
    "    )\n",
    "\n",
    "    train_output = trainer.train()\n",
    "    return trainer, train_output\n",
    "\n",
    "def push_to_hub(trainer: Trainer, tokenizer, repo_id: str, id2label: Dict[int, str], label2id: Dict[str, int]):\n",
    "    \"\"\"Pushes the model, tokenizer, and label mappings to Hugging Face Hub.\"\"\"\n",
    "    trainer.model.config.id2label = id2label\n",
    "    trainer.model.config.label2id = label2id\n",
    "    trainer.model.config.problem_type = \"single_label_classification\"\n",
    "\n",
    "    trainer.model.push_to_hub(repo_id)\n",
    "    tokenizer.push_to_hub(repo_id)\n",
    "\n",
    "    api = HfApi()\n",
    "    with open(\"label_mapping.json\", \"w\") as f:\n",
    "        json.dump({\"id2label\": id2label, \"label2id\": label2id}, f, indent=2)\n",
    "    api.upload_file(path_or_fileobj=\"label_mapping.json\", path_in_repo=\"label_mapping.json\", repo_id=repo_id, repo_type=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca7516",
   "metadata": {
    "id": "e4ca7516"
   },
   "source": [
    "### **4. Data Loading**\n",
    "\n",
    "**Source**\n",
    "\n",
    "The dataset consists of posts/comments labeled into 16 mental-health categories (e.g., *adhd, anxiety, depression,* â€¦).\n",
    "\n",
    "The **cleaned splits** (train/test) were exported during preprocessing to **Apache Feather** files and stored in a Hugging Face Datasets path:\n",
    "\n",
    "* `hf://datasets/pfacouetey/DSTI_Deep_Learning_Project_2025/train.feather`\n",
    "* `hf://datasets/pfacouetey/DSTI_Deep_Learning_Project_2025/test.feather`\n",
    "\n",
    "Each file contains two columns:\n",
    "\n",
    "* `text` â€“ the raw post content (string)\n",
    "* `label` â€“ the class name (string, one of 16 categories)\n",
    "\n",
    "**Loading**\n",
    "\n",
    "We load the data with `pandas.read_feather(...)` into standard `pandas.DataFrame`s (`train_df`, `test_df`).\n",
    "\n",
    "Feather is a columnar, binary format, so it loads much faster than CSV and preserves dtypes without extra parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dea389",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "CLEAN_DATA_PATH = 'pfacouetey/DSTI_Deep_Learning_Project_2025'\n",
    "TRAIN_DATA_PATH = f'hf://datasets/{CLEAN_DATA_PATH}/train.feather'\n",
    "TEST_DATA_PATH = f'hf://datasets/{CLEAN_DATA_PATH}/test.feather'\n",
    "\n",
    "train_data_df = pd.read_feather(TRAIN_DATA_PATH)\n",
    "train_df, val_df = train_test_split(\n",
    "    train_data_df,\n",
    "    test_size=0.10,\n",
    "    random_state=SEED,\n",
    "    stratify=train_data_df[\"label\"]\n",
    ")\n",
    "test_df = pd.read_feather(TEST_DATA_PATH)\n",
    "\n",
    "# Print the shape of the dfs\n",
    "print(f'Train df shape: {train_df.shape}')\n",
    "print(f'Val df shape: {val_df.shape}')\n",
    "print(f'Test df shape: {test_df.shape}')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- train_df Info ---\")\n",
    "train_df.info()\n",
    "\n",
    "print(\"\\n--- train_df Description ---\")\n",
    "display(train_df.describe(include='all'))\n",
    "\n",
    "print(\"\\n--- Missing Values in train_df ---\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "train_df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ZUlVEahaw",
   "metadata": {
    "id": "6c7ZUlVEahaw"
   },
   "source": [
    "**Observation on Class Balance:**\n",
    "\n",
    "As seen from the `value_counts(normalize=True)` output, the training dataset is relatively well-balanced across the 16 mental health categories.\n",
    "\n",
    "This is beneficial for training as it prevents the model from being heavily biased towards majority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fe074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use create_datasets from the Utility functions\n",
    "raw_datasets, label2id, id2label, num_labels = create_datasets(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90seVhhcZ0Eq",
   "metadata": {
    "id": "90seVhhcZ0Eq"
   },
   "source": [
    "### **5. Models Definition, Configuration, Training, Inference on Unseen Data**\n",
    "\n",
    "For this project, we are exploring three different Transformer models as backbones for text classification: RoBERTa-base, DeBERTa-v3-base, and ModernBERT-large.\n",
    "\n",
    "Each of these models offers unique characteristics and has shown strong performance on various natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colab_defaults():\n",
    "    # Quiet noisy integrations\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "colab_defaults()\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a7aade",
   "metadata": {
    "id": "a9a7aade"
   },
   "source": [
    "##### **4.1. RoBERTa-base**\n",
    "\n",
    "**Method: Tokenization & Baseline Model: RoBERTa-base**\n",
    "\n",
    "**Tokenization**\n",
    "- We will use the backboneâ€™s fast tokenizer with `truncation=True`.\n",
    "- We will set `max_length=384` for a balance between capturing sufficient context and computational efficiency.\n",
    "\n",
    "**What it's?**\n",
    "\n",
    "RoBERTa-base (Liu et al., 2019) is a robust general-purpose Transformer encoder pretrained with an improved BERT recipe, including **more data**, **longer training**, **bigger batches**, **dynamic masking**, and **no next-sentence prediction**.\n",
    "\n",
    "For more information, you can refer to the [Hugging Face documentation](https://huggingface.co/docs/transformers/en/model_doc/roberta).\n",
    "\n",
    "**Key specs**\n",
    "- 12 layers Â· hidden size 768 Â· 12 attention heads (~125M params)\n",
    "- Byte-Pair Encoding (BPE) tokenizer; max seq length 512\n",
    "- Pretrained on large, diverse corpora (e.g., BookCorpus + CC-News + OpenWebText + Stories)\n",
    "\n",
    "**Why we use it as a baseline ?**\n",
    "- It's a proven, widely used encoder that **fine-tunes well** on many text tasks.\n",
    "- It has excellent support in ðŸ¤— Transformers (checkpoints, tokenizers, pipelines).\n",
    "\n",
    "**Fine-tuning defaults**\n",
    "- Cross-entropy loss (single-label, 16 classes)\n",
    "- LR â‰ˆ **2e-5**, batch size **16**, epochs **5**, `max_length` **384**\n",
    "- Model selection by **validation macro-F1**, then one final **test** evaluation\n",
    "\n",
    "**Training control**\n",
    "- We will use step-wise evaluation (`eval_strategy=\"epoch\"`) so early stopping can trigger.\n",
    "- `metric_for_best_model=\"macro_f1\"`, `greater_is_better=True`.\n",
    "- We will save the best checkpoint + use a total save limit to manage disk usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46UYwuQDrsN8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your model config\n",
    "cfg = Config(\n",
    "    model_name=\"roberta-base\",\n",
    "    max_length=384,\n",
    "    batch_size=16,\n",
    "    epochs=5,\n",
    "    lr=2e-5,\n",
    "    gradient_accumulation=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    hub_repo_id=\"paragonadey/mh-text-classifier-roberta-base\"\n",
    ")\n",
    "\n",
    "# Use tokenize_datasets from the Utility functions\n",
    "tok_ds, tok, collator = tokenize_datasets(raw_datasets, cfg.model_name, cfg.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EI7SNpx9sAgz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to get this RoBERTa model parameters\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(cfg.model_name, num_labels=num_labels)\n",
    "roberta_params_df, roberta_layer_summary_df, roberta_total_params, roberta_trainable_params = summarize_model_parameters(roberta_model, \"RoBERTa-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff1b93",
   "metadata": {
    "id": "c5ff1b93"
   },
   "source": [
    "**RoBERTa-base Model Summary:**\n",
    "\n",
    "As shown in the output above:\n",
    "\n",
    "*   The RoBERTa-base model has a total of **124,657,936 parameters**, all of which are trainable.\n",
    "*   The parameters are primarily distributed between the `roberta` layers (the main transformer encoder) and the `classifier` layers (the newly added classification head).\n",
    "*   The majority of the parameters reside in the `roberta` layers (124,055,040), while the `classifier` layers, which are newly initialized for this downstream task, contain a smaller number of parameters (602,896).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2b738",
   "metadata": {
    "id": "93f2b738"
   },
   "source": [
    "##### _RoBERTa model training - evaluation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OzeJ93XBwWgr",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Use the train_model function from utility function\n",
    "# This function handles model initialization, training args, trainer setup, and training.\n",
    "\n",
    "# Get training arguments\n",
    "args = get_training_args(cfg, num_labels, id2label, label2id)\n",
    "\n",
    "# Explicitly set metric_for_best_model for EarlyStoppingCallback\n",
    "if cfg.use_early_stopping:\n",
    "    args.metric_for_best_model = \"macro_f1\"\n",
    "    args.load_best_model_at_end = True\n",
    "\n",
    "trainer, train_output = train_model(\n",
    "    cfg.model_name,\n",
    "    num_labels,\n",
    "    id2label,\n",
    "    label2id,\n",
    "    tok_ds,\n",
    "    collator,\n",
    "    cfg,\n",
    "    args\n",
    ")\n",
    "\n",
    "# Plot loss curves using the utility function\n",
    "plot_loss_curves(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0604fb3",
   "metadata": {
    "id": "f0604fb3"
   },
   "source": [
    "- The training process shows that the model's performance improved over the epochs, with both training and validation loss decreasing, and accuracy and macro F1 score increasing.\n",
    "- The training stopped after 3 epochs, likely due to the Early Stopping Callback, which is configured to monitor the validation macro F1 and stop training if it doesn't improve for a certain number of epochs. This helps prevent overfitting and saves computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ZV_wM65Kjhv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set using the updated evaluate_model function\n",
    "train_report, train_accuracy = evaluate_model(\n",
    "    trainer,\n",
    "    tok_ds,\n",
    "    id2label,\n",
    "    num_labels,\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a728ae3c",
   "metadata": {
    "id": "a728ae3c"
   },
   "source": [
    "\n",
    "**Classification Report Summary on Training Set:**\n",
    "\n",
    "- The classification report shows a high overall accuracy of **0.9009** on the training set.\n",
    "- Many classes exhibit strong precision, recall, and F1-scores, indicating that the model has learned to classify these categories well during training.\n",
    "- Notably, classes like `normal`, `EDAnonymous`, and `alcoholism` show very high performance metrics.\n",
    "\n",
    "**Confusion Matrix Observations on Training Set:**\n",
    "\n",
    "*   **Depression â†” Suicidewatch:** There is a significant confusion between these two classes, with a notable number of `depression` posts being misclassified as `suicidewatch` (571 instances) and vice versa (270 instances). This is likely due to the semantic overlap and shared vocabulary related to despair and self-harm.\n",
    "*   **Anxiety Family Collisions:** The model shows confusion among the anxiety-related categories (`anxiety`, `socialanxiety`, `healthanxiety`). This suggests that distinguishing between generic anxiety and more specific forms based on the text content remains challenging.\n",
    "*   **BPD â†” Bipolar:** There's some confusion between Borderline Personality Disorder (`bpd`) and `bipolarreddit`, likely stemming from shared language related to mood swings and relationship difficulties.\n",
    "*   **Lonely â†’ Depression:** A number of `lonely` posts are misclassified as `depression` (123 instances), which is understandable given the frequent co-occurrence and similar phrasing of symptoms.\n",
    "*   **\"Normal\" Class Separation:** The `normal` class is almost perfectly separated, with very few instances being misclassified into other categories, and very few posts from other categories being misclassified as `normal`. This suggests the model is effective at identifying posts that do not fit into the specific mental health categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee203b",
   "metadata": {
    "id": "fbee203b"
   },
   "source": [
    "##### _Save trained RoBERTa model_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a347da1",
   "metadata": {
    "id": "4a347da1"
   },
   "source": [
    "!!! We comment this part since there's restricted access to push to this repository, even if you can load from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B2tepbBuWqUs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Hugging Face Hub\n",
    "# notebook_login()\n",
    "\n",
    "# Push the model, tokenizer, and label mappings to Hugging Face Hub\n",
    "# push_to_hub(trainer, tok, cfg.hub_repo_id, id2label, label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6298e8c",
   "metadata": {
    "id": "d6298e8c"
   },
   "source": [
    "##### _Trained RoBERTa model inference on unseen data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-VnE27Kce6oU",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer from Hugging Face Hub\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(cfg.hub_repo_id) # Use cfg.hub_repo_id\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(cfg.hub_repo_id) # Use cfg.hub_repo_id\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_report, test_accuracy = evaluate_model(\n",
    "    trainer,\n",
    "    tok_ds,\n",
    "    id2label,\n",
    "    num_labels,\n",
    "    split=\"test\"\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f96519",
   "metadata": {
    "id": "53f96519"
   },
   "source": [
    "##### _Comparison of RoBERTa-base Performance on Train and Test Sets_\n",
    "\n",
    "\n",
    "| Metric        | Train Set | Test Set | Observation                                  |\n",
    "| :------------ | :-------- | :------- | :------------------------------------------- |\n",
    "| **Accuracy**  | 0.9009    | 0.7936   | Expected drop in accuracy on unseen data.    |\n",
    "| **Macro F1**  | 0.9008    | 0.7934   | Similar drop in Macro F1, indicating consistent performance across classes. |\n",
    "| **Normal Class F1** | 0.9990    | 0.9955   | The model maintains very high performance on the 'normal' class on unseen data. |\n",
    "| **Depression F1** | 0.7161    | 0.4971   | A significant drop in Depression F1, highlighting challenges with this class on unseen data. |\n",
    "| **Suicidewatch F1** | 0.8299    | 0.6898   | A notable drop in Suicidewatch F1, also indicating difficulty generalizing for this class. |\n",
    "| **Anxiety F1** | 0.8134    | 0.6387   | A substantial decrease in Anxiety F1 on unseen data. |\n",
    "| **Socialanxiety F1** | 0.8676    | 0.7275   | Performance on Socialanxiety also drops on unseen data. |\n",
    "| **Healthanxiety F1** | 0.9457    | 0.8647   | Healthanxiety performance is more robust but still sees a drop. |\n",
    "\n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "*   The performance metrics on the test set are lower than those on the training set, which is expected and indicates that the model has learned the training data well but faces some challenges generalizing to new, unseen data. This is a common sign of potential overfitting, although early stopping helps mitigate this.\n",
    "\n",
    "*   The confusion patterns observed in the training set (e.g., between Depression and Suicidewatch, and within the Anxiety family) appear to persist and are often exacerbated on the test set, as evidenced by the larger performance drops for these specific classes.\n",
    "\n",
    "*   The \"normal\" class continues to be well-classified on the test set, suggesting the model effectively learns to distinguish non-mental health related posts.\n",
    "\n",
    "*   Looking at the confusion matrix on the test set, the model performs strongly on classes like `normal` (with high precision, recall, and F1-score), `EDAnonymous`, `healthanxiety`, `alcoholism`, and `autism`. The diagonal cells for these classes are notably dominant.\n",
    "\n",
    "*   Conversely, certain classes and pairs remain challenging, showing higher rates of confusion:\n",
    "    *   **Depression â†” Suicidewatch:** This is the main confusion pair, with a significant number of `depression` posts misclassified as `suicidewatch` (246 instances) and vice versa (123 instances). `Depression` also shows some leakage to `lonely` (78 instances), which is semantically expected given the overlap in symptoms.\n",
    "    *   **Anxiety Family:** Confusion persists within the anxiety-related classes, with notable misclassifications such as `anxiety` to `socialanxiety` (116 instances) and `anxiety` to `healthanxiety` (80 instances), and some reverse flow between these categories.\n",
    "    *   **BPD â†” Bipolarreddit:** There is continued confusion between `bpd` and `bipolarreddit`, with instances of `bpd` being misclassified as `bipolarreddit` (39) and `bipolarreddit` as `bpd` (42)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378a74e8",
   "metadata": {
    "id": "378a74e8"
   },
   "source": [
    "##### **4.2. DeBERTa-v3-base**\n",
    "\n",
    "**What it is.** DeBERTa (Decoding-enhanced BERT with disentangled attention) improves upon BERT and RoBERTa by using a disentangled attention mechanism and a new masked language model pre-training objective. DeBERTa-v3 is a further improved version. For more information, you can refer to the [Hugging Face documentation](https://huggingface.co/docs/transformers/v4.57.1/en/model_doc/deberta#deberta).\n",
    "\n",
    "**Key specs.**\n",
    "- 12 layers Â· hidden size 768 Â· 12 attention heads (~125M params)\n",
    "- Uses a SentencePiece tokenizer; max seq length 512\n",
    "- Pretrained on a large text corpus\n",
    "\n",
    "**Why use it.**\n",
    "- Achieved state-of-the-art results on various NLP benchmarks, including SuperGLUE.\n",
    "- The disentangled attention mechanism can capture richer interactions between tokens.\n",
    "\n",
    "**My fine-tuning approach.**\n",
    "- Similar fine-tuning parameters as RoBERTa (LR, batch size, max_length).\n",
    "- We will use Early Stopping based on validation macro-F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_85GZX9NDVAX",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for DeBERTa-v3-base model\n",
    "cfg_deberta = Config(\n",
    "    model_name=\"microsoft/deberta-v3-base\",\n",
    "    max_length=384,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    lr=2e-5,\n",
    "    gradient_accumulation=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    hub_repo_id=\"paragonadey/mh-text-classifier-deberta-v3-base_v2\"\n",
    ")\n",
    "\n",
    "set_seed(cfg_deberta.seed)\n",
    "\n",
    "# Use tokenize_datasets from utility function for DeBERTa\n",
    "# raw_datasets, label2id, id2label, num_labels were created in the RoBERTa section and can be reused\n",
    "tok_ds_deberta, tok_deberta, collator_deberta = tokenize_datasets(raw_datasets, cfg_deberta.model_name, cfg_deberta.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gUY_TWFIDiQN",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to get this DeBERTa model parameters\n",
    "deberta_model = AutoModelForSequenceClassification.from_pretrained(cfg_deberta.model_name, num_labels=num_labels)\n",
    "deberta_params_df, deberta_layer_summary_df, deberta_total_params, deberta_trainable_params = summarize_model_parameters(deberta_model, \"DeBERTa-v3-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b0793",
   "metadata": {
    "id": "ef6b0793"
   },
   "source": [
    "As shown in the output above:\n",
    "\n",
    "*   The DeBERTa-v3-base model has a total of **184,434,448 parameters**, all of which are trainable.\n",
    "*   The parameters are primarily distributed among the `deberta` layers (the main transformer encoder), the `pooler` layer, and the `classifier` layers (the newly added classification head).\n",
    "*   The majority of the parameters reside in the `deberta` layers (183,831,552), while the `pooler` layer contains 590,592 parameters and the `classifier` layers, which are newly initialized for this downstream task, contain a smaller number of parameters (12,304)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe864bc",
   "metadata": {
    "id": "bbe864bc"
   },
   "source": [
    "##### _DeBERTa model training - evaluation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fKwOF90sfYYc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Get training arguments for DeBERTa\n",
    "args_deberta = get_training_args(cfg_deberta, num_labels, id2label, label2id)\n",
    "\n",
    "# Explicitly set metric_for_best_model for EarlyStoppingCallback (if used)\n",
    "if cfg_deberta.use_early_stopping:\n",
    "    args_deberta.metric_for_best_model = \"macro_f1\"\n",
    "    args_deberta.load_best_model_at_end = True\n",
    "\n",
    "# Use the train_model function from nlp_utils.py for DeBERTa training\n",
    "trainer_deberta, train_output_deberta = train_model(\n",
    "    cfg_deberta.model_name,\n",
    "    num_labels,\n",
    "    id2label,\n",
    "    label2id,\n",
    "    tok_ds_deberta, # Use DeBERTa tokenized dataset\n",
    "    collator_deberta, # Use DeBERTa data collator\n",
    "    cfg_deberta, # Use DeBERTa config\n",
    "    args_deberta # Use DeBERTa training arguments\n",
    ")\n",
    "\n",
    "# Plot loss curves using the utility function for DeBERTa\n",
    "plot_loss_curves(trainer_deberta.state.log_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26deee0",
   "metadata": {
    "id": "d26deee0"
   },
   "source": [
    "**Training Report (DeBERTa-v3-base):**\n",
    "\n",
    "**Learning Curve:** The training loss decreased quickly initially (from ~2.7, though not shown in this summary table, to ~0.5 by epoch 3) and then continued a slower decline, indicating healthy optimization.\n",
    "\n",
    "**Validation Trend:** The validation metrics, particularly the loss, showed improvement in the early epochs (from ~0.90 down to a minimum near ~0.78 mid-training) but then crept up slightly towards the end of training. Macro F1 and accuracy followed a similar pattern of initial improvement followed by a slight plateau or dip.\n",
    "\n",
    "\n",
    "This trend suggests mild overfitting after the midpoint of training. The model continued to improve its fit on the training data (decreasing training loss), while its ability to generalize to the unseen validation data plateaued and began to slightly worsen (validation loss creeping up). Despite this, the overall training process was stable and the model converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vl3PcAYbfmnU",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the DeBERTa model on the training set.\n",
    "train_report_deberta, train_accuracy_deberta = evaluate_model(\n",
    "    trainer_deberta,\n",
    "    tok_ds_deberta,\n",
    "    id2label,\n",
    "    num_labels,\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b1bb8",
   "metadata": {
    "id": "072b1bb8"
   },
   "source": [
    "**Classification Report Summary on Training Set:**\n",
    "\n",
    "*   **Very strong classes:** Classes with notably high F1-scores include `normal` (â‰ˆ0.999), `EDAnonymous`, `alcoholism`, `autism`, `healthanxiety`, and `ptsd`.\n",
    "*   **Weaker classes:** Classes with lower F1-scores, indicating more difficulty in classification, include `depression` (â‰ˆ0.68), `anxiety` (â‰ˆ0.78), `bpd`/`bipolarreddit` (mid-0.8s), `suicidewatch` (â‰ˆ0.82), and `socialanxiety` (â‰ˆ0.85).\n",
    "\n",
    "**Confusion Matrix Observations on Training Set:**\n",
    "\n",
    "*   **Depression â†” Suicidewatch:** This remains the most significant source of confusion. A large number of `depression` posts are misclassified as `suicidewatch` (595 instances), and vice versa (319 instances). This is expected due to the semantic overlap between these topics.\n",
    "*   **Anxiety Family Mixing:** Confusion persists within the anxiety-related classes. `anxiety` is frequently misclassified as `socialanxiety` (313 instances) and `healthanxiety` (150 instances), with some reverse misclassifications as well.\n",
    "*   **BPD â†” Bipolarreddit:** There is notable confusion between Borderline Personality Disorder (`bpd`) and `bipolarreddit`, with instances of `bpd` being misclassified as `bipolarreddit` (111) and `bipolarreddit` as `bpd` (124). This likely stems from shared language related to mood and instability.\n",
    "*   **Lonely â†’ Depression:** A considerable number of `lonely` posts are misclassified as `depression` (157 instances), which is understandable given the semantic proximity and frequent co-occurrence of these states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af289f26",
   "metadata": {
    "id": "af289f26"
   },
   "source": [
    "##### _Save trained DeBERTa model_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeabda5",
   "metadata": {
    "id": "fbeabda5"
   },
   "source": [
    "!!! We comment this part since there's restricted access to push to this repository, even if you can load from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EMnM-C9jfmf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the DeBERTa model, tokenizer, and label mappings to Hugging Face Hub\n",
    "# push_to_hub(trainer_deberta, tok_deberta, cfg_deberta.hub_repo_id, id2label, label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db4fe3",
   "metadata": {
    "id": "73db4fe3"
   },
   "source": [
    "##### _Trained DeBERTa model inference on unseen data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7td726aRhdVo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the DeBERTa model on the test set using the updated evaluate_model function\n",
    "test_report_deberta, test_accuracy_deberta = evaluate_model(\n",
    "    trainer_deberta, # Use the DeBERTa trainer\n",
    "    tok_ds_deberta, # Use the DeBERTa tokenized dataset\n",
    "    id2label, # Use the id2label mapping\n",
    "    num_labels, # Use the number of labels\n",
    "    split=\"test\" # Specify the test split\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Evaluation Results (DeBERTa Model):\")\n",
    "print(f\"Accuracy: {test_accuracy_deberta:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0c57d",
   "metadata": {
    "id": "c1d0c57d"
   },
   "source": [
    "##### _Comparison of DeBERTa-v3-base Performance on Train and Test Sets_\n",
    "\n",
    "| Metric        | Train Set | Test Set | Observation                                  |\n",
    "| :------------ | :-------- | :------- | :------------------------------------------- |\n",
    "| **Accuracy**  | 0.8846    | 0.7973   | Expected drop in accuracy on unseen data, but still solid generalization. |\n",
    "| **Macro F1**  | 0.8846    | 0.7973   | Similar drop in Macro F1, indicating consistent performance across classes on test set. |\n",
    "| **Normal Class F1** | 0.9986    | 0.9955   | Maintains very high performance on the 'normal' class on unseen data. |\n",
    "| **Depression F1** | 0.6832    | 0.5067   | Significant drop in Depression F1, highlighting challenges with this class on unseen data. |\n",
    "| **Suicidewatch F1** | 0.8155    | 0.6982   | Notable drop in Suicidewatch F1, also indicating difficulty generalizing for this class. |\n",
    "| **Anxiety F1** | 0.7811    | 0.6431   | Substantial decrease in Anxiety F1 on the test set. |\n",
    "| **Socialanxiety F1** | 0.8460    | 0.7252   | Performance on Socialanxiety also drops on the test set. |\n",
    "| **Healthanxiety F1** | 0.9339    | 0.8662   | Healthanxiety performance is more robust but still sees a drop. |\n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "*   Looking at the confusion matrix on the test set, the model performs strongly on classes with F1 scores generally above 0.86, including `normal` (0.996), `EDAnonymous` (0.936), `alcoholism` (0.888), `healthanxiety` (0.866), `autism` (0.878), `ptsd` (0.846), `addiction` (0.862), and `adhd` (0.851).\n",
    "\n",
    "*   Classes with moderate performance (F1 between 0.72 and 0.81) include `bipolarreddit` (0.773), `bpd` (0.744), `lonely` (0.744), `schizophrenia` (0.801), and `socialanxiety` (0.725).\n",
    "\n",
    "*   Certain classes remain particularly challenging for the model to classify accurately on unseen data, showing lower F1 scores: `anxiety` (0.643), `suicidewatch` (0.698), and `depression` (0.507).\n",
    "\n",
    "*   Analysis of the confusion matrix reveals similar confusion patterns as observed on the training set, often exacerbated on the test data:\n",
    "    *   **Depression â†” Suicidewatch:** This is a significant two-way confusion, with approximately 213 `depression` posts misclassified as `suicidewatch` and around 138 `suicidewatch` posts misclassified as `depression`. Additionally, about 81 `depression` posts are misclassified as `lonely`, which is semantically understandable.\n",
    "    *   **Anxiety Family:** Confusion persists among the anxiety-related classes, with notable misclassifications such as `anxiety` to `socialanxiety` (around 104 instances) and `anxiety` to `healthanxiety` (around 77 instances), along with some reverse flow.\n",
    "    *   **BPD â†” Bipolarreddit:** There are swaps in both directions between `bpd` and `bipolarreddit`, with approximately 43 `bpd` posts misclassified as `bipolarreddit` and about 38 `bipolarreddit` posts misclassified as `bpd`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710f191a",
   "metadata": {
    "id": "710f191a"
   },
   "source": [
    "##### **4.3.  ModernBERT-large**\n",
    "\n",
    "**What it is.** ModernBERT is another variant of BERT, aiming for improved performance. The \"large\" version indicates a larger model size compared to base models. For more information, you can refer to the [Hugging Face documentation](https://huggingface.co/docs/transformers/v4.57.1/en/model_doc/modernbert#modernbert).\n",
    "\n",
    "**Key specs.**\n",
    "- Larger number of parameters compared to base models.\n",
    "- Specific architecture details and pre-training may vary.\n",
    "\n",
    "**Why use it.**\n",
    "- Larger models often have higher capacity to learn complex patterns.\n",
    "- I want to compare its performance against established models like RoBERTa and DeBERTa on this specific task.\n",
    "\n",
    "**Fine-tuning approach.**\n",
    "- Similar fine-tuning parameters as the other models.\n",
    "- We will use Early Stopping based on validation macro-F1.\n",
    "- We plan to perform hyperparameter tuning specifically for this model to potentially optimize its performance further.# ModerBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sWu_MzLWp5Cm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for ModernBERT-large model\n",
    "cfg_moderbert = Config(\n",
    "    model_name=\"answerdotai/ModernBERT-large\",\n",
    "    max_length=384, # Keep max_length consistent for comparison\n",
    "    batch_size=16,  # Keep batch_size consistent\n",
    "    epochs=5,       # Start with 5 epochs, early stopping will control actual epochs\n",
    "    lr=2e-5,\n",
    "    gradient_accumulation=2, # Keep consistent\n",
    "    eval_strategy=\"epoch\",\n",
    "    hub_repo_id=\"paragonadey/mh-text-classifier-moderbert-large_v2\" # Set repository ID for ModerBERT model\n",
    ")\n",
    "\n",
    "set_seed(cfg_moderbert.seed) # Set seed for reproducibility with this config\n",
    "\n",
    "# Use tokenize_datasets from nlp_utils for ModerBERT\n",
    "tok_ds_moderbert, tok_moderbert, collator_moderbert = tokenize_datasets(raw_datasets, cfg_moderbert.model_name, cfg_moderbert.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-V3jbA7KHOzg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to get this ModernBERT model parameters\n",
    "moderbert_model = AutoModelForSequenceClassification.from_pretrained(cfg_moderbert.model_name, num_labels=num_labels)\n",
    "moderbert_params_df, moderbert_layer_summary_df, moderbert_total_params, moderbert_trainable_params = summarize_model_parameters(moderbert_model, \"ModernBERT-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7c8de",
   "metadata": {
    "id": "32c7c8de"
   },
   "source": [
    "As shown in the output above:\n",
    "\n",
    "*   The ModernBERT-large model has a total of **395,847,696 parameters**, all of which are trainable.\n",
    "*   The parameters are primarily distributed among the `model` layers (the main transformer encoder), the `head` layer, and the `classifier` layers (the newly added classification head).\n",
    "*   The vast majority of the parameters reside in the `model` layers (394,781,696), while the `head` layer contains 1,049,600 parameters and the `classifier` layers, which are newly initialized for this downstream task, contain a smaller number of parameters (16,400)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff3e5a",
   "metadata": {
    "id": "24ff3e5a"
   },
   "source": [
    "##### _ModerBERT model training - evaluation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mF8hOqiyp43W",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Get training arguments for ModerBERT\n",
    "args_moderbert = get_training_args(cfg_moderbert, num_labels, id2label, label2id)\n",
    "\n",
    "# Explicitly set metric_for_best_model for EarlyStoppingCallback (if used)\n",
    "if cfg_moderbert.use_early_stopping:\n",
    "    args_moderbert.metric_for_best_model = \"macro_f1\"\n",
    "    args_moderbert.load_best_model_at_end = True\n",
    "\n",
    "# Use the train_model function from utility function\n",
    "trainer_moderbert, train_output_moderbert = train_model(\n",
    "    cfg_moderbert.model_name,\n",
    "    num_labels,\n",
    "    id2label,\n",
    "    label2id,\n",
    "    tok_ds_moderbert,\n",
    "    collator_moderbert,\n",
    "    cfg_moderbert,\n",
    "    args_moderbert\n",
    ")\n",
    "\n",
    "# Plot loss curves using the utility function for ModerBERT\n",
    "plot_loss_curves(trainer_moderbert.state.log_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b809f0",
   "metadata": {
    "id": "c3b809f0"
   },
   "source": [
    "**Training Report (ModernBERT-large):**\n",
    "\n",
    "*   **Fast Fit Phase (0 â†’ ~1k steps):** Training loss plunges quickly from approximately 2.9 to 1.0. This indicates that the model rapidly adapts to the task, likely driven by the initial warmup and large gradient updates.\n",
    "*   **Stable Improvement (~1k â†’ ~7k):** Training loss continues a steady decline towards approximately 0.7. Validation points begin appearing around 7k steps, with validation loss around 0.75â€“0.8.\n",
    "*   **Sweet Spot (~7k â†’ ~14.5k):** Training loss continues to decrease slightly (around 0.5â€“0.6), while validation performance remains relatively flat or shows slight improvement. This plateau phase typically represents the point where the best model checkpoint is likely to be found.\n",
    "*   **Overfitting Onset (~14.5k â†’ end):** A clear divergence occurs as training loss drops significantly lower (to around 0.2â€“0.3), but validation loss steadily climbs (from approximately 0.7 to over 1.1). This indicates classic overfitting, where the model is memorizing the training data but losing its ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qAfx1OpNp9vc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the ModerBERT model on the training set\n",
    "train_report_moderbert, train_accuracy_moderbert = evaluate_model(\n",
    "    trainer_moderbert,\n",
    "    tok_ds_moderbert,\n",
    "    id2label,\n",
    "    num_labels,\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d878350",
   "metadata": {
    "id": "7d878350"
   },
   "source": [
    "\n",
    "**Classification Report Summary on Training data**\n",
    "\n",
    "The classification report shows a very high overall accuracy of **0.9857** on the training set. The macro average F1 score is also **0.9856**, indicating excellent and balanced performance across classes on the training data.\n",
    "\n",
    "*   **Very strong classes:** The evaluation shows very high training accuracy, with most classes being learned almost perfectly. Classes like `normal`, `EDAnonymous`, `alcoholism`, `autism`, `healthanxiety`, `ptsd`, `schizophrenia`, and `adhd` exhibit huge diagonal counts in the confusion matrix with minimal misclassification.\n",
    "\n",
    "**Confusion Matrix Observations on the Training data:**\n",
    "\n",
    "*   **Depression â†” Suicidewatch:** There is a notable tail of misclassifications between `depression` and `suicidewatch`. Additionally, some `depression` posts are misclassified as `lonely`.\n",
    "*   **Anxiety Family Mixing:** Confusion persists within the anxiety family, with instances of `anxiety` being misclassified as `socialanxiety` and `healthanxiety`.\n",
    "*   **BPD â†” Bipolarreddit:** There is a modest two-way swap of misclassifications between `bpd` and `bipolarreddit`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33015b0c",
   "metadata": {
    "id": "33015b0c"
   },
   "source": [
    "##### _Save trained ModerBERT model_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dadb40",
   "metadata": {
    "id": "55dadb40"
   },
   "source": [
    "!!! We comment this part since there's restricted access to push to this repository, even if you can load from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ISORCh6Pq0I2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the ModerBERT model, tokenizer, and label mappings to Hugging Face Hub\n",
    "# push_to_hub(trainer_moderbert, tok_moderbert, cfg_moderbert.hub_repo_id, id2label, label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac429ac",
   "metadata": {
    "id": "aac429ac"
   },
   "source": [
    "##### _Trained ModerBERT model inference on unseen data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amDZGlz8rcHo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the ModerBERT model on the test set\n",
    "test_report_moderbert, test_accuracy_moderbert = evaluate_model(\n",
    "    trainer_moderbert,\n",
    "    tok_ds_moderbert,\n",
    "    id2label,\n",
    "    num_labels,\n",
    "    split=\"test\"\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Evaluation Results (ModerBERT Model):\")\n",
    "print(f\"Accuracy: {test_accuracy_moderbert:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26db6ac",
   "metadata": {
    "id": "c26db6ac"
   },
   "source": [
    "##### _Comparison of ModernBERT-large Performance on Train and Test Sets_\n",
    "\n",
    "\n",
    "| Metric        | Train Set | Test Set | Observation                                  |\n",
    "| :------------ | :-------- | :------- | :------------------------------------------- |\n",
    "| **Accuracy**  | 0.9857    | 0.8097   | Expected drop in accuracy on unseen data, but significantly higher than base models. |\n",
    "| **Macro F1**  | 0.9856    | 0.8099   | Significant drop from train, indicating overfitting, but still strong generalization. |\n",
    "| **Normal Class F1** | 0.9999    | 0.9960   | Maintains exceptionally high performance on the 'normal' class on unseen data. |\n",
    "| **Depression F1** | 0.9619    | 0.5394   | Very significant drop in Depression F1, indicating this remains a very challenging class for generalization. |\n",
    "| **Suicidewatch F1** | 0.9772    | 0.6982   | Significant drop in Suicidewatch F1, also indicating difficulty generalizing for this class. |\n",
    "| **Anxiety F1** | 0.9736    | 0.6509   | Substantial decrease in Anxiety F1 on the test set. |\n",
    "| **Socialanxiety F1** | 0.9808    | 0.7386   | Performance on Socialanxiety also drops on the test set. |\n",
    "| **Healthanxiety F1** | 0.9911    | 0.8782   | Healthanxiety performance is more robust but still sees a notable drop. |\n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "*   The confusion matrix on the test set is strongly diagonal, indicating good generalization for many classes. Classes with very high separation, showing large diagonal counts and minimal spillover to other classes, include `normal` (almost perfect), `EDAnonymous`, `healthanxiety`, `alcoholism`, `autism`, `PTSD`, `ADHD`, `addiction`, and `schizophrenia`.\n",
    "\n",
    "*   However, the model still struggles with certain semantically close pairs, where confusion is more pronounced:\n",
    "    *   **Depression â†” Suicidewatch:** This remains the heaviest two-way confusion on the test set. There is a notable block of `depression` posts misclassified as `suicidewatch`, and a significant number of `suicidewatch` posts misclassified as `depression`. Some `depression` posts are also misclassified as `lonely`.\n",
    "    *   **Anxiety Family:** Mixing within the anxiety family persists, with visible misclassifications such as `anxiety` to `socialanxiety` and `anxiety` to `healthanxiety`.\n",
    "    *   **BPD â†” Bipolarreddit:** There are visible swaps in both directions between `bpd` and `bipolarreddit`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ab204",
   "metadata": {
    "id": "482ab204"
   },
   "source": [
    "### **6. Results, Interpretation & Future Works**\n",
    "\n",
    "- We evaluated three Transformer models (RoBERTa-base, DeBERTa-v3-base, and ModernBERT-large) for mental health text classification. As a quick reminder :\n",
    "\n",
    "| Model              | Train Accuracy | Test Accuracy | Train Macro F1 | Test Macro F1 | Total Parameters |\n",
    "| :----------------- | :------------- | :------------ | :------------- | :------------ | :--------------- |\n",
    "| **RoBERTa-base**   | 0.9009         | 0.7936        | 0.9008         | 0.7934        | 124.7M           |\n",
    "| **DeBERTa-v3-base**| 0.8846         | 0.7973        | 0.8846         | 0.7973        | 184.4M           |\n",
    "| **ModernBERT-large**| 0.9857        | 0.8097        | 0.9856         | 0.8099        | 395.8M           |\n",
    "\n",
    "- All models performed reasonably well on the test set (Accuracy ~0.79-0.81).\n",
    "\n",
    "- **ModernBERT-large** achieved the highest test performance but showed more overfitting than base models.\n",
    "\n",
    "- **DeBERTa-v3-base** slightly outperformed **RoBERTa-base**.\n",
    "\n",
    "- Persistent confusion exists between semantically similar classes, particularly **Depression â†” Suicidewatch** and within the **Anxiety family**. The **\"normal\"** class was consistently well-classified.\n",
    "\n",
    "As future works :\n",
    "\n",
    "*   **Hyperparameter tuning** (computationally expensive but potentially beneficial).\n",
    "*   **Ensemble methods** for improved robustness.\n",
    "*   Exploring **alternative model architectures**.\n",
    "*   **Data augmentation** for weaker classes.\n",
    "*   **Specialized tokenization/embeddings** for mental health language.\n",
    "*   **Incorporating external knowledge**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
